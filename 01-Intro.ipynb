{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructor and Administrative Details\n",
    "\n",
    "## Schedule\n",
    "\n",
    "1. Intro, NumPy, Pandas, Numba\n",
    "2. Multiprocessing, Dask, DaskML\n",
    "3. GPU, Rapids, BlazingSQL, Numba, DL/Custom Loss\n",
    "4. Bonus Tools and Patterns\n",
    "\n",
    "## Instructor: Adam Breindel\n",
    "\n",
    "<img src=\"https://materials.s3.amazonaws.com/i/med-head.jpg\" width=200>\n",
    "\n",
    "### Contact\n",
    "* LinkedIn https://www.linkedin.com/in/adbreind\n",
    "* Email adbreind@gmail.com\n",
    "* Twitter `@adbreind` (no cat or breakfast pix, pretty much just tech)\n",
    "* 20+ years building all kinds of systems for startups and large enterprises\n",
    "* 10+ years teaching (data engineering, AI/ML, frontend, backend, mobile)\n",
    "\n",
    "### Interesting projects...\n",
    "* My first full-time job in tech was streaming neural-net fraud scoring\n",
    "* Realtime & offline analytics for banking\n",
    "* Music synchronization and licensing for networked jukeboxes\n",
    "\n",
    "### Industries\n",
    "* Finance, Insurance, Travel, Media / Entertainment, Government, Energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Performance Python\n",
    "\n",
    "### Python: A Systems Approach to High-Performance\n",
    "\n",
    "In 2015, Reynold Xin, Apache Spark's Chief Architect, gave a series of presentations on the second major refactoring of Spark's internals in as many years.\n",
    "\n",
    "A thorough understanding of the new Spark paradigm would require substantial additional knowledge about the changes, and Reynold needed to explain and justify the dramatic overhaul of Spark in the pursuit of higher performance.\n",
    "\n",
    "*Why couldn't Spark be improved bit by bit, fixing hotspots and bottlenecks, to achieve speed in a less disruptive way?*\n",
    "\n",
    "For small speed goals, that might work, Mr. Xin explained. But for substantial speedups that were targeted -- 10x or even 100x -- the arithmetic just doesn't work out.\n",
    "\n",
    "Difficult to get order of magnitude performance speed ups with profiling techniques\n",
    "\n",
    "* For 10x improvement, would need of find top hotspots that add up to 90% and make them instantaneous\n",
    "* For 100x, 99%\n",
    "\n",
    "Instead, look bottom up, how fast should it run?\n",
    "\n",
    "If the goal is to achieve orders-of-magnitude speedup, and fully exploit modern hardware, it is both more efficient and more effective to refactor the underlying mechanics to exploit that that hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ideal Philosophy for Python Perf__\n",
    "\n",
    "Although Spark's refactor was focused on the Scala/JVM ecosystem, Python is even more amenable to that approach.\n",
    "\n",
    "Why?\n",
    "\n",
    "Because one of Python's strengths is its easy integration to native code: the SciPy stack is fundamentally built on leveraging Python as a control language (or interface/automation language) while deferring implementation of expensive work to native code.\n",
    "\n",
    "While there are a few patterns to improve the speed of typical Python code, that approach is not the primary mechanism to achieve speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, numba, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "my_list = list(range(1000000))\n",
    "out = []\n",
    "\n",
    "for i in my_list:\n",
    "    out.append(math.sqrt(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "out = []\n",
    "\n",
    "for i in range(1000000):\n",
    "    out.append(math.sqrt(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "out = [math.sqrt(i) for i in range(1000000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "out = np.sqrt(np.arange(1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def get_roots(vec):\n",
    "    return np.sqrt(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "get_roots(np.arange(1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it's always good practice to try and use a language efficiently, Python is not designed to require or heavily exploit syntax- and idiom-level performance tricks. \n",
    "\n",
    "Instead, Python encourages a straightforward API and idiom placed above a high-performance implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What About Higher-Level Tools/APIs?\n",
    "\n",
    "This pattern -- locating performance-critical functionality, then employing libraries with high-level Python APIs and high-performance, low-level implementations -- is common to many functional domains, not just numeric computation:\n",
    "\n",
    "* Web/REST frameworks (https://falconframework.org/#sectionBenchmarks)\n",
    "* Graph analysis (https://graph-tool.skewed.de/performance)\n",
    "* Structured data I/O (https://www.h5py.org/)\n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreter \"Wars\" and Tradeoffs\n",
    "\n",
    "Over the years, several alternate runtimes have been considered for Python, with various goals.\n",
    "\n",
    "However, for data-instensive computing, the original Python interpreter (\"CPython\") is the only practical solution today (2020).\n",
    "\n",
    "__Why?__\n",
    "\n",
    "Consider...\n",
    "* Jython gains some speed from JVM infrastructure \n",
    "    * but cannot interoperate with Python modules using C extensions (most of the SciPy stack)\n",
    "* PyPy is significantly faster than CPython (https://www.pypy.org/) and is always \"worth a try\"\n",
    "    * but ... has numerious package incompatibilities: http://packages.pypy.org/\n",
    "    * issues include fundamental packages like Pandas and Scikit-learn\n",
    "    \n",
    "The extensibility issues __may__ be addressed in general in the future. Travis Oliphant and Quansight have come out in support of the `epython` spec ... but for app dev, this is likely many years away: https://openteams.com/projects/epython\n",
    "\n",
    "\n",
    "Fundamentally, CPython is necessary for most data-intensive workloads today (2020) and CPython isn't the fastest interpreter out there.\n",
    "\n",
    "Luckily, as we'll see, leveraging the proper libraries obviates those issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
